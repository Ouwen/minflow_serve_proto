// Code generated by protoc-gen-go. DO NOT EDIT.
// source: protobuf/inference.proto

/*
Package inference is a generated protocol buffer package.

It is generated from these files:
	protobuf/inference.proto

It has these top-level messages:
	InferenceTask
	InferenceResult
	MultiInferenceRequest
	MultiInferenceResponse
*/
package inference

import proto "github.com/golang/protobuf/proto"
import fmt "fmt"
import math "math"
import tensorflow_serving2 "github.com/ouwen/minflow_serve_proto/protobuf/classification"
import tensorflow_serving "github.com/ouwen/minflow_serve_proto/protobuf/input"
import tensorflow_serving1 "github.com/ouwen/minflow_serve_proto/protobuf/model"
import tensorflow_serving3 "github.com/ouwen/minflow_serve_proto/protobuf/regression"

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package

// Inference request such as classification, regression, etc...
type InferenceTask struct {
	ModelSpec *tensorflow_serving1.ModelSpec `protobuf:"bytes,1,opt,name=model_spec,json=modelSpec" json:"model_spec,omitempty"`
	// Signature's method_name. Should be one of the method names defined in
	// third_party/tensorflow/python/saved_model/signature_constants.py.
	// e.g. "tensorflow/serving/classify".
	MethodName string `protobuf:"bytes,2,opt,name=method_name,json=methodName" json:"method_name,omitempty"`
}

func (m *InferenceTask) Reset()                    { *m = InferenceTask{} }
func (m *InferenceTask) String() string            { return proto.CompactTextString(m) }
func (*InferenceTask) ProtoMessage()               {}
func (*InferenceTask) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{0} }

func (m *InferenceTask) GetModelSpec() *tensorflow_serving1.ModelSpec {
	if m != nil {
		return m.ModelSpec
	}
	return nil
}

func (m *InferenceTask) GetMethodName() string {
	if m != nil {
		return m.MethodName
	}
	return ""
}

// Inference result, matches the type of request or is an error.
type InferenceResult struct {
	ModelSpec *tensorflow_serving1.ModelSpec `protobuf:"bytes,1,opt,name=model_spec,json=modelSpec" json:"model_spec,omitempty"`
	// Types that are valid to be assigned to Result:
	//	*InferenceResult_ClassificationResult
	//	*InferenceResult_RegressionResult
	Result isInferenceResult_Result `protobuf_oneof:"result"`
}

func (m *InferenceResult) Reset()                    { *m = InferenceResult{} }
func (m *InferenceResult) String() string            { return proto.CompactTextString(m) }
func (*InferenceResult) ProtoMessage()               {}
func (*InferenceResult) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{1} }

type isInferenceResult_Result interface {
	isInferenceResult_Result()
}

type InferenceResult_ClassificationResult struct {
	ClassificationResult *tensorflow_serving2.ClassificationResult `protobuf:"bytes,2,opt,name=classification_result,json=classificationResult,oneof"`
}
type InferenceResult_RegressionResult struct {
	RegressionResult *tensorflow_serving3.RegressionResult `protobuf:"bytes,3,opt,name=regression_result,json=regressionResult,oneof"`
}

func (*InferenceResult_ClassificationResult) isInferenceResult_Result() {}
func (*InferenceResult_RegressionResult) isInferenceResult_Result()     {}

func (m *InferenceResult) GetResult() isInferenceResult_Result {
	if m != nil {
		return m.Result
	}
	return nil
}

func (m *InferenceResult) GetModelSpec() *tensorflow_serving1.ModelSpec {
	if m != nil {
		return m.ModelSpec
	}
	return nil
}

func (m *InferenceResult) GetClassificationResult() *tensorflow_serving2.ClassificationResult {
	if x, ok := m.GetResult().(*InferenceResult_ClassificationResult); ok {
		return x.ClassificationResult
	}
	return nil
}

func (m *InferenceResult) GetRegressionResult() *tensorflow_serving3.RegressionResult {
	if x, ok := m.GetResult().(*InferenceResult_RegressionResult); ok {
		return x.RegressionResult
	}
	return nil
}

// XXX_OneofFuncs is for the internal use of the proto package.
func (*InferenceResult) XXX_OneofFuncs() (func(msg proto.Message, b *proto.Buffer) error, func(msg proto.Message, tag, wire int, b *proto.Buffer) (bool, error), func(msg proto.Message) (n int), []interface{}) {
	return _InferenceResult_OneofMarshaler, _InferenceResult_OneofUnmarshaler, _InferenceResult_OneofSizer, []interface{}{
		(*InferenceResult_ClassificationResult)(nil),
		(*InferenceResult_RegressionResult)(nil),
	}
}

func _InferenceResult_OneofMarshaler(msg proto.Message, b *proto.Buffer) error {
	m := msg.(*InferenceResult)
	// result
	switch x := m.Result.(type) {
	case *InferenceResult_ClassificationResult:
		b.EncodeVarint(2<<3 | proto.WireBytes)
		if err := b.EncodeMessage(x.ClassificationResult); err != nil {
			return err
		}
	case *InferenceResult_RegressionResult:
		b.EncodeVarint(3<<3 | proto.WireBytes)
		if err := b.EncodeMessage(x.RegressionResult); err != nil {
			return err
		}
	case nil:
	default:
		return fmt.Errorf("InferenceResult.Result has unexpected type %T", x)
	}
	return nil
}

func _InferenceResult_OneofUnmarshaler(msg proto.Message, tag, wire int, b *proto.Buffer) (bool, error) {
	m := msg.(*InferenceResult)
	switch tag {
	case 2: // result.classification_result
		if wire != proto.WireBytes {
			return true, proto.ErrInternalBadWireType
		}
		msg := new(tensorflow_serving2.ClassificationResult)
		err := b.DecodeMessage(msg)
		m.Result = &InferenceResult_ClassificationResult{msg}
		return true, err
	case 3: // result.regression_result
		if wire != proto.WireBytes {
			return true, proto.ErrInternalBadWireType
		}
		msg := new(tensorflow_serving3.RegressionResult)
		err := b.DecodeMessage(msg)
		m.Result = &InferenceResult_RegressionResult{msg}
		return true, err
	default:
		return false, nil
	}
}

func _InferenceResult_OneofSizer(msg proto.Message) (n int) {
	m := msg.(*InferenceResult)
	// result
	switch x := m.Result.(type) {
	case *InferenceResult_ClassificationResult:
		s := proto.Size(x.ClassificationResult)
		n += proto.SizeVarint(2<<3 | proto.WireBytes)
		n += proto.SizeVarint(uint64(s))
		n += s
	case *InferenceResult_RegressionResult:
		s := proto.Size(x.RegressionResult)
		n += proto.SizeVarint(3<<3 | proto.WireBytes)
		n += proto.SizeVarint(uint64(s))
		n += s
	case nil:
	default:
		panic(fmt.Sprintf("proto: unexpected type %T in oneof", x))
	}
	return n
}

// Inference request containing one or more requests.
type MultiInferenceRequest struct {
	// Inference tasks.
	Tasks []*InferenceTask `protobuf:"bytes,1,rep,name=tasks" json:"tasks,omitempty"`
	// Input data.
	Input *tensorflow_serving.Input `protobuf:"bytes,2,opt,name=input" json:"input,omitempty"`
}

func (m *MultiInferenceRequest) Reset()                    { *m = MultiInferenceRequest{} }
func (m *MultiInferenceRequest) String() string            { return proto.CompactTextString(m) }
func (*MultiInferenceRequest) ProtoMessage()               {}
func (*MultiInferenceRequest) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{2} }

func (m *MultiInferenceRequest) GetTasks() []*InferenceTask {
	if m != nil {
		return m.Tasks
	}
	return nil
}

func (m *MultiInferenceRequest) GetInput() *tensorflow_serving.Input {
	if m != nil {
		return m.Input
	}
	return nil
}

// Inference request containing one or more responses.
type MultiInferenceResponse struct {
	// List of results; one for each InferenceTask in the request, returned in the
	// same order as the request.
	Results []*InferenceResult `protobuf:"bytes,1,rep,name=results" json:"results,omitempty"`
}

func (m *MultiInferenceResponse) Reset()                    { *m = MultiInferenceResponse{} }
func (m *MultiInferenceResponse) String() string            { return proto.CompactTextString(m) }
func (*MultiInferenceResponse) ProtoMessage()               {}
func (*MultiInferenceResponse) Descriptor() ([]byte, []int) { return fileDescriptor0, []int{3} }

func (m *MultiInferenceResponse) GetResults() []*InferenceResult {
	if m != nil {
		return m.Results
	}
	return nil
}

func init() {
	proto.RegisterType((*InferenceTask)(nil), "tensorflow.serving.InferenceTask")
	proto.RegisterType((*InferenceResult)(nil), "tensorflow.serving.InferenceResult")
	proto.RegisterType((*MultiInferenceRequest)(nil), "tensorflow.serving.MultiInferenceRequest")
	proto.RegisterType((*MultiInferenceResponse)(nil), "tensorflow.serving.MultiInferenceResponse")
}

func init() { proto.RegisterFile("protobuf/inference.proto", fileDescriptor0) }

var fileDescriptor0 = []byte{
	// 383 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xa4, 0x93, 0x4f, 0xaf, 0xd2, 0x40,
	0x14, 0xc5, 0x2d, 0x04, 0x94, 0x69, 0x8c, 0x3a, 0x01, 0x53, 0x48, 0x88, 0x58, 0x5d, 0x74, 0xd5,
	0x26, 0xb8, 0x60, 0x83, 0x1b, 0xdc, 0xe8, 0x02, 0x17, 0x83, 0x89, 0x89, 0x9b, 0xa6, 0x94, 0x5b,
	0x98, 0xd0, 0xce, 0xd4, 0xf9, 0x23, 0x6b, 0x3f, 0xc1, 0xfb, 0xba, 0x6f, 0xf9, 0xc2, 0x0c, 0xed,
	0xe3, 0x4f, 0xf3, 0x36, 0x6f, 0xd7, 0x9c, 0x9c, 0xf3, 0xbb, 0xa7, 0xb7, 0xb7, 0xc8, 0x2b, 0x05,
	0x57, 0x7c, 0xad, 0xb3, 0x88, 0xb2, 0x0c, 0x04, 0xb0, 0x14, 0x42, 0x23, 0x61, 0xac, 0x80, 0x49,
	0x2e, 0xb2, 0x9c, 0x1f, 0x42, 0x09, 0xe2, 0x1f, 0x65, 0xdb, 0xd1, 0xb8, 0x76, 0xa7, 0x79, 0x22,
	0x25, 0xcd, 0x68, 0x9a, 0x28, 0xca, 0x99, 0x8d, 0x8c, 0xfa, 0x67, 0xb0, 0x52, 0xab, 0x1b, 0xb5,
	0xe0, 0x1b, 0xc8, 0x4f, 0xea, 0xb0, 0x56, 0x05, 0x6c, 0x05, 0x48, 0x59, 0x63, 0x7c, 0x86, 0x5e,
	0xff, 0xa8, 0xca, 0xfc, 0x4a, 0xe4, 0x1e, 0xcf, 0x11, 0x32, 0xd1, 0x58, 0x96, 0x90, 0x7a, 0xce,
	0xc4, 0x09, 0xdc, 0xe9, 0x38, 0xbc, 0xed, 0x17, 0x2e, 0x8f, 0xae, 0x55, 0x09, 0x29, 0xe9, 0x15,
	0xd5, 0x23, 0xfe, 0x80, 0xdc, 0x02, 0xd4, 0x8e, 0x6f, 0x62, 0x96, 0x14, 0xe0, 0xb5, 0x26, 0x4e,
	0xd0, 0x23, 0xc8, 0x4a, 0x3f, 0x93, 0x02, 0xfc, 0xbb, 0x16, 0x7a, 0x53, 0x0f, 0x24, 0x20, 0x75,
	0xae, 0x9e, 0x39, 0x32, 0x46, 0x83, 0xcb, 0x05, 0xc5, 0xc2, 0x60, 0xcd, 0x70, 0x77, 0x1a, 0x34,
	0x81, 0xbe, 0x5d, 0x04, 0x6c, 0x8d, 0xef, 0x2f, 0x48, 0x3f, 0x6d, 0xd0, 0xf1, 0x0a, 0xbd, 0x7b,
	0x5c, 0x5b, 0x05, 0x6f, 0x1b, 0xf8, 0xe7, 0x26, 0x38, 0xa9, 0xcd, 0x35, 0xf8, 0xad, 0xb8, 0xd2,
	0x16, 0xaf, 0x50, 0xd7, 0x92, 0xfc, 0xff, 0x0e, 0x1a, 0x2c, 0x75, 0xae, 0xe8, 0xd9, 0x5a, 0xfe,
	0x6a, 0x90, 0x0a, 0xcf, 0x50, 0x47, 0x25, 0x72, 0x2f, 0x3d, 0x67, 0xd2, 0x0e, 0xdc, 0xe9, 0xc7,
	0xa6, 0x61, 0x17, 0x1f, 0x8f, 0x58, 0x3f, 0x8e, 0x50, 0xc7, 0x1c, 0xc5, 0x69, 0x05, 0xc3, 0xe6,
	0x60, 0xa9, 0x15, 0xb1, 0x3e, 0xff, 0x37, 0x7a, 0x7f, 0x5d, 0x41, 0x96, 0x9c, 0x49, 0xc0, 0x5f,
	0xd1, 0x4b, 0xdb, 0xb3, 0x6a, 0xf1, 0xe9, 0xc9, 0x16, 0xf6, 0xed, 0x48, 0x95, 0x59, 0xcc, 0xff,
	0xcc, 0xb6, 0x54, 0xed, 0xf4, 0x3a, 0x4c, 0x79, 0x11, 0x71, 0x7d, 0x00, 0x16, 0x15, 0x94, 0x1d,
	0xc3, 0xf1, 0x31, 0x0c, 0xb1, 0x39, 0xc3, 0xe8, 0xf6, 0xcf, 0xb8, 0x77, 0x9c, 0x75, 0xd7, 0xc8,
	0x5f, 0x1e, 0x02, 0x00, 0x00, 0xff, 0xff, 0x3b, 0x3c, 0x1b, 0x07, 0x39, 0x03, 0x00, 0x00,
}
